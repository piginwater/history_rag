{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    "    set_global_handler\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext, StorageContext\n",
    "from custom.llms.GeminiLLM import Gemini\n",
    "from custom.llms.proxy_model import ProxyModel\n",
    "from llama_index.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.postprocessor import SentenceTransformerRerank\n",
    "# llm = Gemini(temperature=0.01, model_name=\"gemini-pro\", max_tokens=2048)\n",
    "llm = ProxyModel(temperature=0.01, model_name=\"qwen:7b\", max_tokens=2048)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-zh-v1.5\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "set_global_service_context(service_context)\n",
    "rerank_postprocessor = SentenceTransformerRerank(\n",
    "            model=\"BAAI/bge-reranker-large\", top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.law_sentence_window import LawSentenceWindowNodeParser\n",
    "documents = SimpleDirectoryReader(\"./data/law/\").load_data()\n",
    "node_parser = LawSentenceWindowNodeParser.from_defaults(\n",
    "            window_size=3,\n",
    "            window_metadata_key=\"window\",\n",
    "            original_text_metadata_key=\"original_text\",)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import ChatPromptTemplate, ChatMessage, MessageRole, PromptTemplate\n",
    "\n",
    "QA_SYSTEM_PROMPT = \"你是一个严谨的中国法律学的专家，你会仔细阅读包含法律条文的资料并给出准确的回答,你的回答都会非常准确\"\n",
    "QA_PROMPT_TMPL_STR = (\n",
    "    \"请你仔细阅读给出的法律法规资料进行回答，如果发现资料无法得到答案，就回答不知道。 \\n\"\n",
    "    \"仅根据以下搜索出的相关法律法规资料，而不依赖于先验知识，回答问题。\\n\"\n",
    "    \"每个法律法规资料，都有三个附加信息，1. 该资料所属法律法规；2. 该资料在法律法规中所属的章节；3. 该资料在法律法规中所属的章节中所属的条款。回答法律法规条款内容时，请参考\\n\"\n",
    "    \"搜索的相关法律法规资料如下所示。\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"问题: {query_str}\\n\"\n",
    "    \"答案: \"\n",
    ")\n",
    "REFINE_PROMPT_TMPL_STR = ( \n",
    "    \"你是一个严谨的中国法律学的专家，你严格按以下方式工作\"\n",
    "    \"1.只有原答案为不知道时才进行修正,否则输出原答案的内容。\\n\"\n",
    "    \"2.如果感到疑惑的时候，就用原答案的内容回答。\\n\"\n",
    "    \"新的相关法律法规资料: {context_msg}\\n\"\n",
    "    \"原问题: {query_str}\\n\"\n",
    "    \"原答案: {existing_answer}\\n\"\n",
    "    \"新答案: \"\n",
    ")\n",
    "\n",
    "message_templates = [\n",
    "            ChatMessage(content=QA_SYSTEM_PROMPT, role=MessageRole.SYSTEM),\n",
    "            ChatMessage(\n",
    "                content=QA_PROMPT_TMPL_STR,\n",
    "                role=MessageRole.USER,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=20,node_postprocessors=[\n",
    "            rerank_postprocessor,\n",
    "            MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "        ])\n",
    "chat_template = ChatPromptTemplate(message_templates=message_templates)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": chat_template,\"response_synthesizer:refine_template\": PromptTemplate(REFINE_PROMPT_TMPL_STR)}\n",
    ")\n",
    "query_engine.update_prompts(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set_global_handler(\"simple\")\n",
    "# question=\"某家超市在促销活动中将产品价格先提高，再打折，实际上消费者并没有得到实际的优惠。违反了消费者权益保护法的哪些条款\"\n",
    "#question=\"消费者权益保护法中关于商品价格，有哪些条款\"\n",
    "# question=\"消费者权益保护法第八条的内容\"\n",
    "question=\"消费者权益保护法第二章的详细内容是什么\"\n",
    "#question=\"消费者权益保护法第56条的详细内容是什么\"\n",
    "response = query_engine.query(question)\n",
    "query_engine._get_prompts\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
